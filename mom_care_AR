#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Jun 21 08:45:54 2020

@author: ebrahim
"""

import re
import numpy as np
from nltk.stem.isri import ISRIStemmer 
from sklearn.metrics.pairwise import cosine_similarity 
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer , TfidfTransformer ,CountVectorizer
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics.pairwise import linear_kernel
import csv
import warnings
from sklearn.metrics import accuracy_score ,confusion_matrix,classification_report 
import seaborn as sns
import nltk
# nltk.download("all")
from nltk.corpus import stopwords
import string
from nltk.tokenize import word_tokenize,sent_tokenize
import string
from string import digits
import codecs
#from IPython.core import payload
import json
import http.client

data_set=pd.read_csv(r'/Users/ebrahiMoGedamy/Documents/NLP/nlpTasks/MomcareAR/data.csv')
data_set

filterWords = []
words = []

with codecs.open(r"/Users/ebrahiMoGedamy/Documents/NLP/nlpTasks/MomcareAR/data.csv", encoding="utf-8")as f:
    dataTableSet=f.read()

for item in word_tokenize(dataTableSet):
    re.sub('\n', '', item)
    re.sub('', '', item)
    re.sub('<*?&.;,>', '', item)
    re.sub(r'[٠-٩]+','' , item)
    re.sub(r'[0-9]+','' , item)
    re.sub(r'\s*[A-Za-z]+\b', '' , item)
    if item not in words:
        words.append(item)

text = " "
text = text.join(words)  
# print(text)


st = ISRIStemmer()
for a in words:
    st.stem(a)
    
print("\n                 ========== Remove StepWords =============\n")
sw = stopwords.words('arabic')
# stopped_tokens = [i for i in words if not i in sw]  
print("\n=======================Lemmatization================================")

conn = http.client.HTTPSConnection("farasa-api.qcri.org") 
payload = "{\"text\":\"%s\"}"% text
payload = payload.encode('utf-8')
headers = { 'content-type': "application/json", 'cache-control': "no-cache", }
conn.request("POST", "/msa/webapi/lemma", payload, headers)
res = conn.getresponse()
data = res.read().decode('utf-8')
data_dict = json.loads(data)
# print(data_dict['result'])


X = data_set['sympotems']
y = data_set['class']
z = data_set['remed']
t = data_set['causes']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

vec = CountVectorizer()
# vec=vec.fit(X_train)
X_train_counts=vec.fit_transform(X_train)
# print(pd.DataFrame(vec.transform(X_train_counts).toarray(), columns=sorted(vec.vocabulary_.keys())))

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

# # analyzer=u'word',lowercase=True,stop_words=set(sw)
tfidf = TfidfVectorizer()
X_train_final = tfidf.fit_transform(X_train)

text_clf = Pipeline([('tfidf', TfidfVectorizer()),
                      ('clf', LinearSVC()),
                    ])
text_clf.fit(X_train, y_train)
predictions = text_clf.predict(X_test)


print(accuracy_score(y_test, predictions))
print(confusion_matrix(y_test, predictions))
print(classification_report(y_test, predictions))
# print(pd.DataFrame(vec.transform(X_train_final).toarray(), columns=sorted(vec.vocabulary_.keys())))

#     cosine = cosine_similarity(X_train_final[index], X_train_final)
#     print(cosine)
#     print("\n============")

def get_class(sympotem):
    
    data_set['id'] = range(1, len(data_set) + 1)
    tfidf_matrix_input = tfidf.fit_transform([sympotem])
    tfidf_matrix_sympotems = tfidf.fit_transform(data_set['sympotems'])
    tfidf_matrix_input = tfidf.transform([sympotem])
    tfidf_matrix_sympotems = tfidf.transform(data_set['sympotems'])
    cosine_sim = linear_kernel(tfidf_matrix_input,tfidf_matrix_sympotems)
    sympotems_indices = pd.Series(data_set.index, index=data_set['id']).drop_duplicates()
    sim_scores = list(enumerate(cosine_sim[0]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) 
    ranked_indices=([i[0] for i in sim_scores])
    idClass=data_set['class'].iloc[ranked_indices]
    idSympotems=data_set['sympotems'].iloc[ranked_indices]
    idRemed=data_set['remed'].iloc[ranked_indices]
    idCauses=data_set['causes'].iloc[ranked_indices]
    cl="%s" % (idClass[0])
    r="%s" % (idRemed[0])
    c="%s" % (idCauses[0])
    return (cl,r,c)
print("\n============\n")
print(get_class("نزلات البرد"))









